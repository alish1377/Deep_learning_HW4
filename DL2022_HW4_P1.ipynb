{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4WFRs6jJpfig",
        "0CLr_Pv58jqD",
        "qbLCgTVFboBY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###DEEP LEARNING\n",
        "###HW4.1"
      ],
      "metadata": {
        "id": "4WFRs6jJpfig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/bentrevett/pytorch-seq2seq.git"
      ],
      "metadata": {
        "id": "7Ejtazlj6kmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "16ZxxPAPY-Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import shap\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "K2yrA5XlprQ5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(21)\n",
        "np.random.seed(21)\n",
        "torch.manual_seed(21)\n",
        "torch.cuda.manual_seed(21)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "7TbAgorvQRqH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/ferdousi.txt') as f: poems = [line.strip() for line in f.readlines()]\n",
        "poems = poems[2:]"
      ],
      "metadata": {
        "id": "1LdkwI1Z-2_E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_part = []\n",
        "second_part = []\n",
        "for i in range(len(poems)):\n",
        "  if i % 2 == 0:\n",
        "    first_part.append(poems[i])\n",
        "  else:\n",
        "    second_part.append(poems[i])\n",
        "first_part, second_part = first_part[0:49608], second_part[0:49608]"
      ],
      "metadata": {
        "id": "xShIqRrWmtRK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_part =enumerate(first_part)\n",
        "second_part =enumerate(second_part)"
      ],
      "metadata": {
        "id": "v_Ydp31DbVER"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def build_vocab(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)"
      ],
      "metadata": {
        "id": "y7kQ9kFMc2uN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_generator = build_vocab([first_part, second_part])\n",
        "vocab = build_vocab_from_iterator(token_generator,\n",
        "                                  min_freq=1,\n",
        "                                  specials=['<unk>', '<start>', '<end>'])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "ab2WsOxydJp8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\"پارسیان و من\")\n",
        "indexes = vocab(tokens)\n",
        "\n",
        "tokens, indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKM3EYtxeDSN",
        "outputId": "51dde7cc-c732-43c0-8d98-19576f69eb8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['پارسیان', 'و', 'من'], [0, 3, 23])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\"<eos>\")\n",
        "indexes = vocab(tokens)\n",
        "\n",
        "print(tokens, indexes)\n",
        "print(vocab[\"<sos>\"])\n",
        "print(vocab.get_itos()[18010])\n",
        "print(vocab.get_itos()[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PasEnqtp8L9Z",
        "outputId": "8849a43b-51f6-40fc-d28e-675303fdc5b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<eos>'] [0]\n",
            "0\n",
            "ییک\n",
            "و\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_part = []\n",
        "second_part = []\n",
        "for i in range(len(poems)):\n",
        "  if i % 2 == 0:\n",
        "    first_part.append(poems[i])\n",
        "  else:\n",
        "    second_part.append(poems[i])\n",
        "first_part, second_part = first_part[0:49608], second_part[0:49608]"
      ],
      "metadata": {
        "id": "9s2-JGQi_Mys"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_TV, X_test, y_TV, y_test = train_test_split(first_part, second_part, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_TV, y_TV, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "fx-pvejfbTCJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data, val_data = [], [], []\n",
        "for i in range(len(X_train)):\n",
        "  train_data.append([X_train[i], y_train[i]])\n",
        "for i in range(len(X_test)):\n",
        "  test_data.append([X_test[i], y_test[i]])\n",
        "for i in range(len(X_val)):\n",
        "  val_data.append([X_val[i], y_val[i]])"
      ],
      "metadata": {
        "id": "XjCjVo-HbVBX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train length\", len(train_data),\"\\ntest length\", len(test_data), \"\\nval length\", len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el4FVCfnCVVF",
        "outputId": "9cbbc310-16b8-4255-e962-2ca5ffe68b5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train length 29764 \n",
            "test length 9922 \n",
            "val length 9922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vb(b):\n",
        "    IP, OP = list(zip(*b))\n",
        "    IP = [vocab(tokenizer(S)) for S in IP]\n",
        "    IP = [S+([0]* (9-len(S))) if len(S)<9 else S[:9] for S in IP]\n",
        "    OP = [vocab(tokenizer(sample)) for sample in OP]\n",
        "    OP = [S+([0]* (9-len(S))) if len(S)<9 else S[:9] for S in OP]\n",
        "    IPP,OPP = [], []\n",
        "    for i in IP:\n",
        "      ii = list(i)\n",
        "      ii.insert(0,1)\n",
        "      ii.append(2)\n",
        "      IPP.append(ii)\n",
        "    for j in OP:\n",
        "      jj = list(j)\n",
        "      jj.insert(0,1)\n",
        "      jj.append(2)\n",
        "      OPP.append(jj)\n",
        "    return torch.tensor(IPP, dtype=torch.int32).transpose(0, 1).to(device),torch.tensor(OPP, dtype=torch.int32).transpose(0, 1).to(device)"
      ],
      "metadata": {
        "id": "_6jqijMMaYuQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data, val_data = to_map_style_dataset(train_data), to_map_style_dataset(test_data), to_map_style_dataset(val_data)\n",
        "train_loader = DataLoader(train_data, batch_size=1024, collate_fn=vb)\n",
        "test_loader  = DataLoader(test_data, batch_size=1024, collate_fn=vb)\n",
        "val_loader  = DataLoader(val_data, batch_size=1024, collate_fn=vb)"
      ],
      "metadata": {
        "id": "GL2V45Wpeb6p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "RCmyOTxUw8RI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5uYzRaZxmbI",
        "outputId": "5b872133-0d3f-42ce-9bfd-efc276de63f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for X, Y in train_loader:\n",
        "    print(X.shape, Y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M73L_oFH2Cqa",
        "outputId": "05cc9ec4-6e23-434f-f64a-40a67bce1121"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 1024]) torch.Size([11, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch[0][:,0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiFkNJRr2ahg",
        "outputId": "7c4237a1-f063-4afc-857b-46202d6d6762"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A"
      ],
      "metadata": {
        "id": "0CLr_Pv58jqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, id, ed, hd, nl, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "        self.hd = hd\n",
        "        self.nl = nl\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.Embddng = nn.Embedding(id, ed)\n",
        "        self.RNN = nn.LSTM(ed, hd, nl, dropout = dropout)\n",
        "        \n",
        "    def forward(self, source):\n",
        "        \n",
        "        #source = [source len, batch size]\n",
        "        embedded = self.dropout(self.Embddng(source))\n",
        "        #embedded = [source len, batch size, emb dim]\n",
        "        outputs, (hidden, cell) = self.RNN(embedded)\n",
        "        #outputs = [source len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        #outputs are always from the top hidden layer\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "ODcx_cKE22jd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class decoder(nn.Module):\n",
        "    def __init__(self, od, ed, hd, nl, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "        self.od = od\n",
        "        self.hd = hd\n",
        "        self.nl = nl\n",
        "        self.Embddng = nn.Embedding(od, ed)\n",
        "        self.fc = nn.Linear(hd, od)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.probs = nn.Softmax(dim=1)\n",
        "        self.RNN = nn.LSTM(ed, hd, nl, dropout = dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.Embddng(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.RNN(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return self.probs(prediction), hidden, cell"
      ],
      "metadata": {
        "id": "OxlFJolCWVVd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MODEL(nn.Module):\n",
        "    def __init__(self, encdr, dcdr, dvce):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encdr = encdr\n",
        "        self.dcdr = dcdr\n",
        "        self.device = dvce\n",
        "        \n",
        "        assert encdr.hd == dcdr.hd, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encdr.nl == dcdr.nl, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, sorce, target, ratio = 0.5):\n",
        "\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = target.shape[1]\n",
        "        trg_len = target.shape[0]\n",
        "        vocab_size = self.dcdr.od\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encdr(sorce)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = target[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.dcdr(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            tforce = random.random() < ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = target[t] if tforce else top1\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "X2ptYMNNZaco"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MODEL(encoder(len(vocab), 256, 512, 2, 0.5), decoder(len(vocab), 256, 512, 2, 0.5), device).to(device)"
      ],
      "metadata": {
        "id": "7LZoZO8FaB_o"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_M4rvJjae6K",
        "outputId": "9044380d-e83a-4ff7-f945-687765d46b4c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MODEL(\n",
              "  (encdr): encoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (Embddng): Embedding(18011, 256)\n",
              "    (RNN): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (dcdr): decoder(\n",
              "    (Embddng): Embedding(18011, 256)\n",
              "    (fc): Linear(in_features=512, out_features=18011, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (probs): Softmax(dim=1)\n",
              "    (RNN): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "hrubb8izatDI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOSS = nn.CrossEntropyLoss(ignore_index = vocab.get_stoi()[\"<unk>\"]).to(device)"
      ],
      "metadata": {
        "id": "xF7EZykaaxXw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(first_batch[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N53u5sV4Uw6s",
        "outputId": "440fc152-3c49-46da-ea95-e18724146da2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_loader, LOSS):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(val_loader):\n",
        "\n",
        "            src, trg = batch\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:,].reshape(output.shape[0])\n",
        "            trg = trg.type(torch.LongTensor).to(device)\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "            loss = LOSS(output, trg)\n",
        "            correct += (output.argmax(1) == trg).type(torch.float).sum().item()\n",
        "            epoch_loss += loss.item()\n",
        "    correct /= trg.shape[0]\n",
        "    return correct , epoch_loss / len(val_loader)"
      ],
      "metadata": {
        "id": "7DCnT1C8b708"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, LOSS, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "        src, trg = batch\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "\n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        output_dim = output.shape[-1]\n",
        "        # print(\"out\", output.shape)\n",
        "        # print(\"argmax\", output.argmax(2).shape)\n",
        "        output = output[1:,].view(-1, output_dim)\n",
        "        trg = trg[1:,].reshape(output.shape[0])\n",
        "\n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        # print(\"out\", trg.shape)\n",
        "        trg = trg.type(torch.LongTensor).to(device)\n",
        "        loss = LOSS(output, trg)\n",
        "        correct += (output.argmax(1) == trg).type(torch.float).sum().item()\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    correct /= trg.shape[0]\n",
        "    return correct, epoch_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "XHDbQs8tWjMa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_v_loss = float(100000)\n",
        "train_history = []\n",
        "val_history = []\n",
        "for epoch in range(4):\n",
        "    t_acc, t_loss = train(model, train_loader, optimizer, LOSS, 1)\n",
        "    v_acc, v_loss = evaluate(model, val_loader, LOSS)\n",
        "    train_history += [t_loss]\n",
        "    val_history += [v_loss]\n",
        "    if v_loss < best_v_loss:\n",
        "        best_v_loss = v_loss\n",
        "        torch.save(model.state_dict(), 'lstmmodel.pt')\n",
        "    print(f'train loss: {t_loss:.3f}\\ntrain acc: {t_acc:.3f}')\n",
        "    print(f'validation loss: {v_loss:.3f}\\nvalidation accuracy: {v_acc:.3f}')\n",
        "    print(\"===========================\\n==============================\")"
      ],
      "metadata": {
        "id": "ZJBA90zzcDx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b870918-d568-4e2d-e498-76cedb743620"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 9.692\n",
            "train acc: 41.763\n",
            "validation loss: 9.624\n",
            "validation accuracy: 1.664\n",
            "===========================\n",
            "==============================\n",
            "train loss: 9.598\n",
            "train acc: 59.497\n",
            "validation loss: 9.593\n",
            "validation accuracy: 1.951\n",
            "===========================\n",
            "==============================\n",
            "train loss: 9.593\n",
            "train acc: 60.765\n",
            "validation loss: 9.593\n",
            "validation accuracy: 1.951\n",
            "===========================\n",
            "==============================\n",
            "train loss: 9.593\n",
            "train acc: 60.765\n",
            "validation loss: 9.593\n",
            "validation accuracy: 1.951\n",
            "===========================\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = range(4)\n",
        "plt.plot(e, train_history)\n",
        "plt.plot(e, val_history)\n",
        "plt.title(\"validation and train loss curves\")\n",
        "plt.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "ho5FQdyqSN1_",
        "outputId": "3250f31a-b28e-4d2b-d2d0-f4206cd0c235"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B"
      ],
      "metadata": {
        "id": "qbLCgTVFboBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class encdr(nn.Module):\n",
        "\n",
        "    def __init__(self, id, ed, hd, nl, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "        self.hd = hd\n",
        "        # self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(id, ed)\n",
        "        self.gru = nn.GRU(ed, hd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, suorce):\n",
        "        \n",
        "        #suorce = [suorce len, batch size]\n",
        "        embedded = self.dropout(self.embedding(suorce))\n",
        "        #embedded = [suorce len, batch size, emb dim]\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        #outputs = [suorce len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        #outputs are always from the top hidden layer\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "uukZqk5mboyy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dcdr(nn.Module):\n",
        "    def __init__(self, od, ed, hd, nl, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "        self.od = od\n",
        "        self.hd = hd\n",
        "        # self.n_layers = n_layers\n",
        "        self.Embddng = nn.Embedding(od, ed)\n",
        "        self.gru = nn.GRU(ed + hd, hd)\n",
        "        self.fc = nn.Linear(ed + hd * 2, od)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.probs = nn.Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, input, hidden, z):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.Embddng(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        ec = torch.cat((embedded, z), dim = 2)\n",
        "        output, hidden = self.gru(ec, hidden)\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), z.squeeze(0)), dim = 1)\n",
        "        prediction = self.fc(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "metadata": {
        "id": "BGNXPlFcby3A"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MODEL(nn.Module):\n",
        "    def __init__(self, encdr, dcdr, dvce):\n",
        "        super().__init__()\n",
        "        self.encdr = encdr\n",
        "        self.dcdr = dcdr\n",
        "        self.device = dvce\n",
        "    def forward(self, sorce, trget, ratio = 0.5):\n",
        "\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher forcing ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trget.shape[1]\n",
        "        vocab_size = self.dcdr.od\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trget.shape[0], batch_size, vocab_size).to(self.device)\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        Z = self.encdr(sorce)\n",
        "        hidden = Z\n",
        "        #first input to the decoder is the <start> tokens\n",
        "        input = trget[0,:]\n",
        "        \n",
        "        for t in range(1, trget.shape[0]):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden=self.dcdr(input, hidden, Z)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trget[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "JJYHCaU8dt4Y"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MODEL(encdr(len(vocab), 256, 512, 2, 0.5),\n",
        "              dcdr(len(vocab), 256, 512, 2, 0.5),\n",
        "              device).to(device)"
      ],
      "metadata": {
        "id": "YcmhcIp0fMlA"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = vocab.get_stoi()[\"<unk>\"]).to(device)"
      ],
      "metadata": {
        "id": "FSvJ6CgafZ59"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_v_loss = float(100000)\n",
        "train_history = []\n",
        "val_history = []\n",
        "for epoch in range(4):\n",
        "    t_acc, t_loss = train(model, train_loader, optimizer, criterion, 1)\n",
        "    v_acc, v_loss = evaluate(model, val_loader, criterion)\n",
        "    train_history += [t_loss]\n",
        "    val_history += [v_loss]\n",
        "    if v_loss < best_v_loss:\n",
        "        best_v_loss = v_loss\n",
        "        torch.save(model.state_dict(), 'lstmmodel.pt')\n",
        "    print(f'train loss: {t_loss:.3f}\\ntrain acc: {t_acc:.3f}')\n",
        "    print(f'validation loss: {v_loss:.3f}\\nvalidation accuracy: {v_acc:.3f}')\n",
        "    print(\"===========================\\n==============================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ9wBvVrf3mQ",
        "outputId": "9fbc100e-ed77-4c6d-fa4a-cd4190022511"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 6.483\n",
            "train acc: 51.351\n",
            "validation loss: 5.960\n",
            "validation accuracy: 1.879\n",
            "===========================\n",
            "==============================\n",
            "train loss: 5.707\n",
            "train acc: 60.882\n",
            "validation loss: 5.813\n",
            "validation accuracy: 1.932\n",
            "===========================\n",
            "==============================\n",
            "train loss: 5.469\n",
            "train acc: 63.440\n",
            "validation loss: 5.681\n",
            "validation accuracy: 1.998\n",
            "===========================\n",
            "==============================\n",
            "train loss: 5.159\n",
            "train acc: 67.750\n",
            "validation loss: 5.569\n",
            "validation accuracy: 2.089\n",
            "===========================\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "همانطور که از دوبخش الف و ب مشاهده شد استفاده از جی.آر.یو و یک تکنیک دیگر منجر به کاهش لاس و افرایش دقت شد"
      ],
      "metadata": {
        "id": "eq0XoFdj8B9S"
      }
    }
  ]
}